{"EventId":212,"EventType":"Custom","EventName":"ARIMA_Trend_Prediction","EventDescription":" Enter EventDescription","Deletable":1,"DomainId":-1,"UpdateDate":"2022-01-17T07:51:15.000Z","UpdatedBy":"Mostafa","CreationDate":"2022-01-09T23:20:08.000Z","CreatedBy":"Mostafa","Lock":{"Username":"Mostafa"},"EventParameters":[{"ParameterId":29585,"EventId":212,"ParameterName":"Code","ParameterValue":"const tf = require('@tensorflow/tfjs')\n\n\nfunction flat(arr, depth = 1) {\n\treturn depth > 0 ? arr.reduce((acc, val) =>\n\t\tacc.concat(Array.isArray(val) ? flat(val, depth - 1)\n\t\t\t: val), [])\n\t\t: arr.slice();\n}\nArray.prototype.flat = function(depth=1) {\n  return flat(this, depth);\n}\n\nArray.prototype.at = function(loc) {\n  if (loc < 0) {\n    loc = this.length + loc;\n  }\n  return this[loc];\n}\n\n\nfunction DataListAddAsync(name, value) {\n  return new Promise((resolve, reject) => {\n      const dataListObj = {\n        name: name,\n        value: value,\n        insertAt: \"tail\"\n      }\n\n      ;\n      DataList.add(dataListObj, dataList_callback);\n\n      function dataList_callback(err, res) {\n        if (err) {\n          return reject(err);\n        }\n\n        resolve(res);\n      }\n    }\n\n  );\n}\n\nfunction DataListRemoveAsync(name, value) {\n  return new Promise((resolve, reject) => {\n      const opt = {\n        \"name\": name,\n        \"value\": value,\n        \"count\": \"0\"\n      };\n\n      DataList.remove(opt, dataList_callback);\n\n      function dataList_callback(err, res) {\n        if (err) {\n          return reject(err);\n        }\n\n        resolve(res);\n      }\n    }\n\n  );\n}\n\nfunction DataListgetAsync(name) {\n  return new Promise((resolve, reject) => {\n      const opt = {\n        \"name\": name\n      };\n\n      DataList.get(opt, dataList_callback);\n\n      function dataList_callback(err, res) {\n        if (err) {\n          return reject(err);\n        }\n\n        resolve(res)\n      }\n    }\n\n  );\n}\n\nfunction DataListupdateAsync(name, value, index) {\n  return new Promise((resolve, reject) => {\n      const opt = {\n        \"name\": name,\n        \"value\": value, // updated value\n        \"index\": index // updated index\n      };\n\n      DataList.update(opt, dataList_callback);\n\n      function dataList_callback(err, res) {\n        if (err) {\n          return reject(err);\n        }\n\n        resolve(res);\n      }\n    }\n\n  );\n}\n\n\nfunction SearchInAsync(query) {\n  return new Promise((resolve, reject) => {\n      // SearchIn(query, searchIn_callback);\n      ExecuteQueryModified(query, searchIn_callback, '');\n\n      function searchIn_callback(err, result) {\n        if (err) {\n          return reject(err);\n        }\n\n        // write your code here\n        resolve(result);\n      }\n    }\n\n  );\n}\n\n/**\n * naive compare array\n * https://stackoverflow.com/questions/3115982/how-to-check-if-two-arrays-are-equal-with-javascript/16430730\n * \n * @param {Array} that \n * @returns \n */\nArray.prototype.equalsTo = function(that) {\n  return JSON.stringify(this) == JSON.stringify(that);\n}\n\n/**\n * \n */\nclass NDArray extends Array {\n  get T() {\n    return transpose(this);\n  }\n\n  get shape() {\n    return shape(this);\n  }\n\n  get ndim() {\n    return ndim(this);\n  }\n\n  get strides() {\n    var size = this.shape.slice(1);\n    size.push(1);\n    for (let i = size.length - 2; i >= 0; i--) {\n      size[i] *= size[i + 1];\n    }\n    return size;\n  }\n\n  /**\n   * \n   * @param  {...any} args \n   * @returns \n   */\n  at(...args) {\n    if (args.length > this.ndim) {\n      throw new Error(\"Index out of bound\");\n    }\n    switch (ndim(args)) {\n      case 1:\n        // var res = this.slice();\n        var res = [...this.slice()];\n        args.forEach(idx => {\n          // res = res[idx];\n          res = res.at(idx);\n        });\n        return res;\n      default:\n        var res = [];\n        if (args.length > 1) {\n          res = this.at(args[0]);\n          args = args.slice(1);\n          args.forEach(idx => {\n            res = res.map(el => [...array(el).at(idx)]);\n          });\n          return res;\n        }\n        [args] = args;\n        args.forEach(i => {\n          // res.push(super.at(i));\n          res.push(this[i]);\n        });\n        return array(res);\n    }\n  }\n}\n\n/**\n * a: array-like or iterable\n * @param {Array} a \n * @returns \n */\nfunction array(a) {\n  return NDArray.from(a);\n}\n\n/**\n * \n * @param {number|NDArray} size \n * @returns \n */\nfunction empty(size) {\n  if (typeof(size) === \"number\") {\n    return new NDArray(size);\n  }\n  if (size instanceof Array) {\n    let p = 1;\n    p = size.flat(size.length - 1)\n      .reduce((p, el) =>\n        p * el\n      );\n    return reshape(Array(p), size);\n  }\n}\n\n/**\n * \n * @param {number|Array} size \n * @returns \n */\nfunction zeros(size) {\n  if (typeof(size) === \"number\") {\n    size = [size]\n  }\n  return reshape(\n    empty(size)\n    .flatten()\n    .fill(0),\n    size);\n}\n\n/**\n * \n * @param {number|Array} size \n * @returns \n */\nfunction ones(size) {\n  if (typeof(size) === \"number\") {\n    size = [size]\n  }\n  return reshape(\n    empty(size)\n    .flatten()\n    .fill(1),\n    size);\n}\n\n/**\n * \n * @param {NDArray} vector \n * @param {number} k \n * @returns \n */\nfunction diag(vector, k = 0) {\n  // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax\n  let res = [];\n  switch (ndim(vector)) {\n    case 1:\n      res = vector.length + Math.abs(k);\n      res = Array(2).fill(res);\n      res = zeros(res);\n      for (let i = Math.max(0, -k), j = Math.max(0, k), l = 0; l < vector.length; i++, j++, l++) {\n        res[i][j] = vector[l];\n      }\n      return res;\n    case 2:\n      let lim = Math.min(...shape(vector));\n      res = empty(lim);\n      for (let i = 0; i < lim; i++) {\n        res[i] = vector[i][i];\n      }\n      return res;\n    default:\n      throw Error(\"Array must be 1D or 2D\");\n  }\n}\n\n/**\n * create an identity matrix\n * @param {number} size \n * @returns \n */\nfunction eye(size) {\n  return diag(ones(size));\n}\n\n/**\n * returns vector[i+1] - vector[i]\n * @param {NDArray} vector \n * @param {number} order \n * @returns \n */\nfunction diff(vector, order = 1) {\n  if (order == 0) {\n    return vector;\n  }\n  // let self = array(vector);\n  let self = [...vector];\n  for (let d = 0; d < order; d++) {\n    let other = self.slice(0, -1);\n    self = self.slice(1);\n    self = self.map((s, idx) =>\n      s - other[idx]\n    );\n  }\n  // return self;\n  return array(self);\n}\n\n/**\n * TODO does not work with axis\n * @param {Array|NDArray} vector \n * @param {number} axis\n * @returns \n */\nfunction cumsum(vector, axis = null) {\n  var total = 0;\n  return vector.map((el) => total += el);\n}\n\n/**\n * TODO axis works only for 2D\n * @param {Array|NDArray} vector\n * @param {number} axis \n * @returns \n */\nfunction mean(vector, axis = null) {\n  // vector=np.array(vector)\n  if (axis != null) {\n    // TODO supporting only 2D\n    // return sum(array(vector), axis).div(vector[axis].length);\n    return sum(array(vector), axis).div(vector.shape[axis]);\n  }\n  return sum(array(vector)) / prod(vector.shape);\n}\n\n/**\n * \n * @param {Array|NDArray} vector \n * @returns \n */\nfunction std(vector, axis = null) {\n  var mu = mean(vector, axis);\n  if (axis != null) {\n    // TODO supports only 2D\n    return sum(\n      array(vector).sub(mu).power(2),\n      axis\n    ).div(vector.shape[axis]).power(.5);\n  }\n  const sigma2 = sum(array(vector).sub(mu).power(2)) / prod(vector.shape);\n  return Math.sqrt(sigma2);\n}\n\n/**\n * \n * @param {Array|NDArray} vector \n * @param {number} axis \n * @returns \n */\nfunction prod(vector, axis = null) {\n  vector = array(vector);\n  if (axis != null) {\n    // TODO support higher dimension prod\n  }\n  return vector.flatten().reduce((p, el) => p * el, 1);\n}\n\n/**\n * number of dimensions of the array\n * @param {Array|NDArray} vector \n * @returns \n */\nfunction ndim(vector) {\n  let dim = 0;\n  let self = [...vector];\n  for (dim = 0; self instanceof Array; dim++) {\n    self = self[0]; // FIXME array elements are not required to be the same here\n  }\n  return dim;\n}\n\n/**\n * FIXME transposing only 2D\n * https://stackoverflow.com/questions/17428587/transposing-a-2d-array-in-javascript\n * @param {NDArray} vector \n * @returns \n */\nfunction transpose(vector) {\n  vector = array(vector);\n  var self = vector.flatten();\n  var res = [];\n  var temp = [];\n  var steps = vector.strides.slice(0, -1);\n  steps.forEach(step => {\n    for (let o = 0; o < step; o++) {\n      for (let i = o; i < self.length; i += step) {\n        temp.push(self[i]);\n      }\n      res.push(temp);\n      temp = [];\n    }\n    [self, res] = [res, []];\n  });\n  return array(self);\n}\n\n/**\n * https://stackoverflow.com/questions/10237615/get-size-of-dimensions-in-array\n * @param {Array|NDArray} vector \n * @returns \n */\nfunction shape(vector) {\n  let self = [...vector];\n  const n = ndim(vector);\n  let shape = [vector.length]\n  for (let dim = 1; dim < n; dim++) {\n    shape.push(self[0].length);\n    self = self[0];\n  }\n  return shape;\n}\n\n/**\n * reshapes the array into the given shape, if possible\n * @param {Array|NDArray} vector \n * @param {Array} size \n * @returns \n */\nfunction reshape(vector, size) {\n  if (typeof(size) === \"number\") {\n    size = [size];\n  }\n  if (size.map(el =>\n      el == -1\n    ).reduce((tot, el) =>\n      tot + el\n    ) > 1) {\n    throw Error(\"Cannot infer more than one dimension\");\n  }\n  let tSize = 1;\n  tSize = shape(vector)\n    .reduce((tSize, el) =>\n      tSize * el\n    );\n  let oSize = 1;\n  oSize = size.reduce((oSize, el) =>\n    oSize * el\n  );\n  if (oSize < 0) {\n    switch (tSize % oSize) {\n      case 0:\n        let index = size.indexOf(-1);\n        size[index] = -tSize / oSize;\n        break;\n      default:\n        throw Error(\"Unable to infer missing dimension\");\n    }\n  } else if (tSize != oSize) {\n    throw Error(\"Incompatible shapes\");\n  }\n  // FIXME keeping the largest array as ndarray, & internal arrays \n  // as normal arrays\n  vector = [...array(vector).flatten()];\n  // vector = [...vector];\n  let result = [];\n  size = size.reverse();\n  for (let idx = 0; idx < size.length - 1; idx++) {\n    let step = size[idx];\n    for (let i = 0; i < vector.length; i += step) {\n      result.push(vector.slice(i, i + step));\n    }\n    vector = result;\n    result = [];\n  }\n  // FIXME had to restore the original order for size\n  size = size.reverse();\n  return array(vector);\n}\n\n/**\n * TODO continue if needed\n * https://numpy.org/doc/stable/reference/generated/numpy.sum.html\n * https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce\n * @param {NDArray} vector \n * @param {number} axis \n * @param {number} initialValue \n * @returns \n */\nfunction sum(vector, axis = null, initialValue = 0) {\n  vector = array(vector);\n  if ((axis === null) || (ndim(vector) == 1)) {\n    // full array or 1D array\n    return vector.flatten()\n      .reduce(((sum, el) =>\n          sum + el\n        ),\n        initialValue);\n  }\n  // TODO supporting 2D sum\n  vector = (axis) ? vector : transpose(vector);\n  return vector.map(el => sum(el));\n  // if ((axis < 0) && (-axis <= this.length)) {\n  // \taxis = this.length - axis\n  // }\n}\n\n/**\n * FIXME implement higher dimensions\n * @param {NDArray} a \n * @param {NDArray} b \n * @returns \n */\nfunction dot(a, b) {\n  [a, b] = [array(a), array(b)];\n  const shapeA = a.shape;\n  const shapeB = b.shape;\n  // const shapeA = shape(a);\n  // const shapeB = shape(b);\n  if (shapeA.at(-1) != shapeB[0]) {\n  // if (shapeA[shapeA.length-1] != shapeB[0]) {\n    throw Error(\"Internal dimension mismatch\");\n  }\n  // vector dot product\n  if ((a.ndim == 1) && (b.ndim == 1)) {\n    return sum(a.mul(b));\n  }\n  if (a.ndim == 1) {\n    a = a.reshape([1, -1]);\n  }\n  if (b.ndim == 1) {\n    b = b.reshape([-1, 1]);\n  }\n  var resShape = [...shapeA.slice(0, -1), ...shapeB.slice(1)];\n  var res = a.reshape([-1, b.shape[0]]);\n  b = transpose(b);\n  res = res.map(row =>\n    b.map(col =>\n      sum(array(row).mul(col))\n    )\n  );\n  return reshape(res, resShape);\n}\n\n/**\n * check if given shapes allow for broadcasting\n * @param {Array} a \n * @param {Array} b \n * @returns \n */\nfunction canBroadcast(a, b) {\n  let [i, j] = [\n    [...a].reverse(), [...b].reverse()\n  ];\n  [i, j] = (i.length >= j.length) ? [i, j] : [j, i];\n  var res = true;\n  j.map((el, idx) =>\n    res &= (el == i[idx]) | (el == 1) | (i[idx] == 1)\n  );\n  return res;\n}\n\n/**\n * \n * @param {Array} size0 \n * @param {Array} size1 \n * @returns \n */\nfunction resultantShape(size0, size1) {\n  matchDimensions(size0, size1);\n  return size0.map((el, i) =>\n    Math.max(el, size1[i])\n  );\n}\n\n/**\n * \n * @param {Array} a \n * @param {Array} b \n */\nfunction matchDimensions(a, b) {\n  var size = a.length - b.length;\n  if (size < 0) {\n    a.unshift(...ones(-size));\n  } else if (size > 0) {\n    b.unshift(...ones(size));\n  }\n}\n\n/**\n * https://numpy.org/doc/stable/user/basics.broadcasting.html\n * @param {NDArray} vector \n * @param {Array} size \n * @returns \n */\nfunction broadcast(vector, size) {\n  vector = array(vector);\n  let vSize = shape(vector);\n  if (!canBroadcast(vSize, size)) {\n    throw Error(\"Can not broadcast\");\n  }\n  matchDimensions(vSize, size);\n  vector = vector.flatten();\n  var tempSize = [];\n  for (let i = size.length - 1; i >= 0; i--) {\n    if (vSize[i] == size[i]) {\n      tempSize.shift();\n      tempSize.unshift(-1, size[i]);\n      vector = reshape(vector, tempSize);\n    } else {\n      vector = vector.map(el =>\n        new NDArray(size[i]).fill(el)\n      );\n      tempSize = shape(vector);\n    }\n  }\n  return reshape(vector, size);\n}\n\n/**\n * https://stackoverflow.com/questions/3895478/does-javascript-have-a-method-like-range-to-generate-a-range-within-the-supp\n * @param {number} start \n * @param {number} end \n * @param {number} step \n * @returns \n */\nfunction arange(start, end, step) {\n  if (end === undefined) {\n    end = start;\n    start = 0;\n  }\n  step = step || 1;\n  if ((end < start) && (step > 0)) {\n    return [];\n  }\n  [start, end] = (start < end) ? [start, end] : [end, start];\n  var res = array(Array(end).keys())\n    .slice(start)\n    .filter(el =>\n      !((el - start) % step)\n    );\n  return (step >= 0) ? res : res.reverse();\n}\n\n\n/**\n * \n * @param {number} start \n * @param {number} stop \n * @param {number} num \n * @returns \n */\nfunction linspace(start, stop, num = 50) {\n  let step = (stop - start) / (num - 1);\n  let res = [];\n  for (let element = start; element < stop; element += step) {\n    // https://stackoverflow.com/questions/2221167/javascript-formatting-a-rounded-number-to-n-decimals\n    res.push(parseFloat(element.toFixed(8)));\n  }\n  return array(res);\n}\n\n/**\n * FIXME works for 2D arrays only\n * @param {Array} elements \n * @returns \n */\nfunction vstack(elements) {\n  let res = [];\n  elements.forEach(el => {\n    el = (el.ndim == 1) ? el : [...el];\n    res.push(...el);\n  });\n  return array(res);\n}\n\n/**\n * FIXME works for 2D arrays only\n * @param {Array} elements \n * @returns \n */\nfunction hstack(elements) {\n  var res;\n  // FIXME edge case\n  if (elements.every(el => ndim(el) == 1)) {\n    res = [];\n    elements.forEach(el => {\n      res.push(...[...el])\n    });\n    return array(res);\n  }\n  res = elements.map(el => transpose(el));\n  return transpose(vstack(res));\n}\n\n/**\n * \n * @param {CallbackFn} op \n * @param  {...any} args \n * @returns \n */\nNDArray.prototype.apply = function(op, ...args) {\n  const size = shape(this);\n  if (args.length) {\n    var n = args.reduce((m, el) =>\n      m + (typeof(el) === \"number\"), 0);\n    if (n == args.length) {\n      return reshape(this.flatten().map(el =>\n        op(el, ...args)\n      ), size);\n    }\n    args = transpose(args.map(el =>\n      broadcast((el instanceof Array) ? el : [el], size).flatten()\n    ));\n    return reshape(this.flatten().map((el, i) => op(el, ...args[i])), size);\n  } else {\n    return reshape(this.flatten().map(el => op(el)), size);\n  }\n}\n\n/**\n * https://stackoverflow.com/questions/7135874/element-wise-operations-in-javascript\n * @param {NDArray|number} that \n * @param {callbackfn} op \n * @returns \n */\nNDArray.prototype.iOperation = function(that, op) {\n  let shapeThis = shape(this);\n  if (typeof(that) === \"number\") {\n    return reshape(\n      this.flatten()\n      .map(el =>\n        op(el, that)\n      ),\n      shapeThis\n    );\n  }\n  that = array(that);\n  let shapeThat = shape(that);\n  if (shapeThis.equalsTo(shapeThat)) {\n    that = that.flatten();\n    return reshape(\n      this.flatten()\n      .map((el, i) =>\n        op(el, that[i])\n      ),\n      shapeThis\n    );\n  }\n  // handling broadcasting\n  shapeThis = resultantShape(shapeThis, shapeThat);\n  that = broadcast(that, shapeThis).flatten();\n  return reshape(\n    broadcast(this, shapeThis).flatten()\n    .map((el, i) =>\n      op(el, that[i])\n    ),\n    shapeThis);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.add = function(that) {\n  return this.apply((a, b) => a + b, that);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.mul = function(that) {\n  return this.apply((a, b) => a * b, that);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.sub = function(that) {\n  return this.apply((a, b) => a - b, that);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.div = function(that) {\n  return this.apply((a, b) => a / b, that);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.equals = function(that) {\n  return this.apply((a, b) => a == b, that);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.power = function(that) {\n  return this.apply((a, b) => a ** b, that);\n}\n\n/**\n * \n * @returns \n */\nNDArray.prototype.flatten = function() {\n  return this.flat(ndim(this) - 1);\n}\n\n/**\n * \n * @returns \n */\nNDArray.prototype.toArray = function() {\n  const s = this.shape;\n  return [...reshape(this.flatten(), s)];\n}\n\n/**\n * \n * @param {Array} size \n * @returns \n */\nNDArray.prototype.reshape = function(size) {\n  return reshape(this, size);\n}\n\n/**\n * \n * @param {NDArray} that \n * @returns \n */\nNDArray.prototype.dot = function(that) {\n  return dot(this, that);\n}\n\n/**\n * \t\n * @returns \n */\nNDArray.prototype.sum = function(axis = null, initialValue = 0) {\n  return sum(this, axis, initialValue);\n}\n\n/**\n * \n * @returns \n */\nNDArray.prototype.mean = function(axis = null) {\n  return mean(this, axis);\n}\n\n/**\n * \n * @returns \n */\nNDArray.prototype.std = function(axis = null) {\n  return std(this, axis);\n}\n\n/**\n * \n * @param {number} axis \n * @returns \n */\nNDArray.prototype.prod = function(axis = null) {\n  return prod(this, axis);\n}\n\nconst linalg = {\n  /**\n   * \n   * @param {NDArray} vector \n   * @param {number} ord \n   * @returns \n   */\n  norm: function(vector, ord = 2) {\n    // FIXME edge cases\n    if (ord === Infinity) {\n\n    } else if (ord === -Infinity) {\n\n    } else if (ord == 0) {\n\n    }\n    return sum(vector.power(ord)) ** (1 / ord)\n  },\n  /**\n   * \n   * @param {Array|NDArray} c \n   * @param {Array|NDArray} r \n   * @returns \n   */\n  toeplitz: function(c, r = null) {\n    if (r) {\n      // TODO handle row not null\n    } else {\n      r = [...c].reverse();\n      var l = c.length;\n      return c.map((_, i) => [...r.slice(-1 - i), ...c.slice(1, l - i)]);\n    }\n  }\n}\n\nconst random = {\n  /**\n   * \n   * @param {Array} size \n   * @returns \n   */\n  random: function(size) {\n    if (!size) {\n      return Math.random();\n    }\n    size = (typeof(size) === 'number') ? [size] : size;\n    return reshape(empty(size).flatten()\n      .map(_ =>\n        Math.random()\n      ),\n      size);\n  }\n}\n////////////////////////////////////////////////////////////////////////////////////////////\nnp = {\n  array,\n  empty,\n  diff,\n  dot,\n  ndim,\n  reshape,\n  shape,\n  sum,\n  transpose,\n  diag,\n  ones,\n  zeros,\n  eye,\n  arange,\n  vstack,\n  hstack,\n  NDArray,\n  linalg,\n  linspace,\n  random,\n  cumsum,\n  mean,\n  std,\n  prod\n}\n\nclass GradientDescent {\n  /**\n   * \n   * @param {number} learningRate \n   * @param {object} kwargs \n   */\n  constructor(learningRate = 0.001, kwargs = {}) {\n    this._alpha = learningRate;\n    this._W = kwargs[\"weights\"];\n    this._gamma = kwargs[\"momentum\"] | 0;\n    this._b = kwargs[\"batchSize\"];\n    this._costFn = kwargs[\"costFunction\"];\n    if (!this._costFn) {\n      this._costFn = function(labels, predictions, m = null) {\n        m = m | labels.length;\n        return np.sum(labels.sub(predictions).power(2)) / m;\n      };\n    }\n    // FIXME gradient is d[cost]/d_W, so it should be different for each cost function\n    this._grad = kwargs[\"gradient\"];\n    if (!this._grad) {\n      this._grad = function(X, y, yHat) {\n        var error = yHat.sub(y);\n        return np.dot(np.transpose(X), error);\n      }\n    }\n    // TODO nesterov update\n    this._update = (kwargs[\"nesterov\"]) ? this.updateNesterov : function(gradient, m, vt1 = 0) {\n      this._W = this._W.sub(this.vt(gradient, m, vt1));\n    };\n  }\n\n  set alpha(learningRate) {\n    this._alpha = learningRate;\n  }\n\n  get alpha() {\n    return this._alpha;\n  }\n\n  set gamma(momentum) {\n    this._gamma = momentum;\n  }\n\n  get gamma() {\n    return this._gamma;\n  }\n\n  get _coef() {\n    return this._W;\n  }\n\n  /**\n   * \n   * @param {NDArray} X \n   * @returns \n   */\n  evaluate(X) {\n    return np.dot(X, this._W);\n  }\n\n  updateNesterov(X, y, m, vt1) {\n    // TODO implement nesterov's update\n    throw Error(\"Method not implemented yet\")\n  }\n\n  vt(gradient, m, vt1 = 0) {\n    return gradient.mul(this._alpha / m)\n      .add(this._gamma * vt1);\n  }\n\n  /**\n   * \n   * @param {NDArray} X \n   * @returns \n   */\n  async predict(X) {\n    var features = X.slice();\n    if (np.ndim(features) == 1) {\n      features = np.reshape(features, [-1, 1]);\n    }\n    features = [np.ones([features.length, 1]), features];\n    features = np.hstack(features);\n    return this.evaluate(features);\n  }\n\n  fitSync(X, y, maxIter = 1024, stopThreshold = 1e-6) {\n    // let ut = 0 // TODO support adaptive grad\n    var costOld;\n    ({\n      costOld,\n      y,\n      X\n    } = this._fitInit(X, y));\n    for (let epoch = 0; epoch < maxIter; epoch++) {\n      var {\n        costCurrent,\n        gradient\n      } = this._runEpoch(X, y);\n      if (this._converged(costOld, costCurrent, stopThreshold, gradient)) {\n        break;\n      } else {\n        costOld = costCurrent;\n      }\n    }\n  }\n\n  _runEpoch(X, y) {\n    var end, batchX, batchY, batchPreds, batchGrad;\n    for (let start = 0; start < y.length; start += this._b) {\n      end = start + this._b;\n      batchX = X.slice(start, end);\n      batchY = y.slice(start, end);\n      batchPreds = this.evaluate(batchX);\n      batchGrad = this._grad(batchX, batchY, batchPreds);\n      // TODO add nesterov update\n      this._update(batchGrad, (this._b > 1) ? this._b : y.length);\n    }\n    var costCurrent = this._costFn(batchY, batchPreds, this._b);\n    return {\n      costCurrent,\n      gradient: batchGrad\n    };\n  }\n\n  _fitInit(X, y) {\n    var nRows = y.length;\n    this._b = this._b || nRows;\n    // FIXME\n    // var costOld = this._costFn(y.slice(-this._b), np.zeros([this._b]), this._b);\n    var costOld = 0;\n    X = np.hstack([np.ones([X.length, 1]), X]);\n    y = np.reshape(y, [nRows, 1]);\n    if (!this._W) {\n      this._W = np.random.random([np.shape(X)[1], 1]);\n    }\n    return {\n      costOld,\n      y,\n      X\n    };\n  }\n\n  _converged(costOld, costCurrent, stopThreshold, gradient) {\n    return !Math.abs(parseInt((costOld - costCurrent) / stopThreshold)) ||\n      !parseInt(np.linalg.norm(gradient) / stopThreshold);\n  }\n\n  /**\n   * \n   * @param {NDArray} X \n   * @param {Array|NDArray} y \n   * @param {number} maxIter \n   * @param {number} stopThreshold \n   * @returns \n   */\n  async fit(X, y, maxIter = 1024, stopThreshold = 1e-6) {\n    // TODO flag to tell fit is done\n\n    this.fitSync(X, y, maxIter, stopThreshold);\n\n    return this;\n  }\n}\n\nclass AutoRegressionIntegratedMovingAverage extends GradientDescent {\n\n  constructor(order = [1, 0, 0], KWArgs = {\n    learningRate: 1e-3\n  }) {\n    super(KWArgs.learningRate || 1e-3, KWArgs);\n    [this._p, this._d, this._q] = order;\n    this._update = function(gradient, m, vt1 = 0) {\n      this._W = this._W.add(this.vt(gradient, m, vt1));\n    };\n  }\n\n  get p() {\n    return this._p;\n  }\n\n  get q() {\n    return this._q;\n  }\n\n  get d() {\n    return this._d;\n  }\n\n  get intercept() {\n    return this.mu * (1 - this.phi.sum());\n  }\n\n  get theta() {\n    return this._W.slice(-this._q);\n  }\n\n  get phi() {\n    return this._W.slice(0, this._p);\n  }\n\n  /**\n   * FIXME should be static\n   * @param {Array} X \n   * @param {number} n \n   * @returns \n   */\n  static _buildNLags(X, n = 0) {\n    return n ? np.linalg.toeplitz(X)\n      .at(\n        np.arange(n, X.length),\n        np.arange(1, n + 1),\n      ) : [];\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} X \n   * @param {number} nCols \n   * @returns \n   */\n  static _buildPredictors(X, nCols) {\n    var predictors = [];\n    for (let idx = 1; idx <= nCols; idx++) {\n      predictors.push(...X.slice(nCols - idx, -idx));\n    }\n    return np.reshape(predictors, [-1, nCols]);\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} X \n   * @param {Array|NDArray} y \n   */\n  score(X, y) {\n    // TODO to be implemented\n    throw new Error(\"Not Implemented yet!\");\n  }\n\n  stat(X) {\n    // TODO trying to take statistics into account\n    var {\n      labels,\n      lags,\n      residuals\n    } = this._fitInit(np.array(X));\n    var costOld = 0;\n    var end, batchX, batchY, batchPredictions, gradient, features, error;\n    for (let start = 0; start < labels.length; start += this._b) {\n      features = np.hstack([\n        lags,\n        AutoRegressionIntegratedMovingAverage._buildNLags(residuals, this._q),\n      ]);\n      end = start + this._b;\n      batchX = features.slice(start, end);\n      batchY = labels.slice(start, end);\n      batchPredictions = super.evaluate(batchX);\n      error = batchY.sub(batchPredictions);\n      residuals.splice(start - labels.length, error.length, ...error);\n      gradient = batchX.mul(error).add(this._W);\n      var muG = gradient.mean();\n      var sigG = gradient.std();\n      console.log(`muG:${muG};sigG=${sigG}`);\n      gradient = batchX.T.dot(error);\n      // TODO add nesterov update\n      this._update(gradient, (this._b > 1) ? this._b : labels.length);\n    }\n    var costCurrent = this._costFn(batchY, batchPredictions, this._b);\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} X \n   * @param {number} maxIter \n   * @param {number} stopThreshold \n   */\n  fitSync(X, maxIter = 32, stopThreshold = 1e-6) {\n    // initialise the fit subroutine\n    var {\n      labels,\n      lags,\n      residuals\n    } = this._fitInit(np.array(X));\n    // initialise cost\n    var costOld = 0;\n    // loop for specified epoch number\n    for (let epoch = 0; epoch < maxIter; epoch++) {\n      // run a single epoch, get the final cost & gradient\n      var {\n        costCurrent,\n        gradient\n      } = this._runEpoch(labels, lags, residuals);\n      // early stopping condition\n      if (super._converged(costOld, costCurrent, stopThreshold, gradient)) {\n        break;\n      } else {\n        // update cost for next epoch\n        costOld = costCurrent;\n      }\n    }\n    this._calculateMetrics(labels, residuals);\n    // keeping residuals for forecasting purposes\n    if (this._q) {\n      this._residuals = residuals.slice(-this._q);\n    }\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} X \n   * @returns \n   */\n  _fitInit(X, initialResiduals = undefined) {\n    /**\n     * random initialise for weights\n     * pros: could converge faster, worst case scenario is to take as much as \n     * zero initialisation\n     * cons: easy to stuck at local minima\n     */\n    this._W = this._W || np.random.random([this._p + this._q]);\n    var yPrime = X.slice();\n    // keeping values for integration step\n    this._initialValue = [];\n    // difference the data\n    for (let d = 0; d < this._d; d++) {\n      this._initialValue.push(yPrime.at(-1 - this._p));\n      yPrime = np.diff(yPrime);\n    }\n    // TODO models could be without mean/constant\n    this.mu = yPrime.mean();\n    yPrime = yPrime.sub(this.mu);\n    // build lags AKA feature vector for AR\n    const lags = AutoRegressionIntegratedMovingAverage._buildNLags(yPrime, this._p);\n    // set labels\n    var labels = yPrime.slice(this._p || this._q);\n    // determine the batch size\n    this._b = this._b || labels.length;\n    // initialise residuals\n    // var residuals = tf.randomNormal(\n    // \tlabels.shape,\n    // \tlabels.mean(),\n    // \tlabels.std()\n    // ).arraySync();\n    var residuals = np.random.random(labels.shape)\n    if (initialResiduals) {\n      residuals.unshift(...initialResiduals);\n    } else if (this._p) {\n      residuals.unshift(...np.zeros(this._q));\n    } else {\n      labels = labels.slice(this._q);\n    }\n    residuals = np.array(residuals);\n    // keep lags for forecasting\n    this._lags = labels.slice(-this._p || labels.length);\n    return {\n      labels,\n      lags,\n      residuals\n    };\n  }\n\n  /**\n   * \n   * @param {np.NDArray} labels \n   * @param {np.NDArray} feats \n   * @param {np.NDArray} residuals \n   * @returns \n   */\n  _runEpoch(labels, feats, residuals) {\n    // define variables required\n    var end, batchX, batchY, batchPredictions, gradient, features, error;\n    // loop over the data in batches; if _b == labels.length -> Vanilla GD; _b == 1 -> stochastic GD\n    for (let start = 0; start < labels.length; start += this._b) {\n      // combine lags & residuals into a single matrix for vectorised operation\n      features = np.hstack([\n        feats,\n        AutoRegressionIntegratedMovingAverage._buildNLags(residuals, this._q),\n      ]);\n      // calculate indices\n      end = start + this._b;\n      // slicing data as batches\n      batchX = features.slice(start, end);\n      batchY = labels.slice(start, end);\n      // predicting by batch\n      batchPredictions = super.evaluate(batchX);\n      // calculating error AKA residual\n      error = batchY.sub(batchPredictions);\n      // updating residuals vector\n      residuals.splice(start - labels.length, error.length, ...error);\n      // calculating gradient\n      gradient = batchX.T.dot(error);\n      // TODO add nesterov update\n      this._update(gradient, (this._b > 1) ? this._b : labels.length);\n      // FIXME\n      // gradient = batchX.mul(error).add(this._W);\n      // var muG = gradient.mean();\n      // var sigG = gradient.std();\n      // console.log(`muG:${muG}\\tsigG=${sigG}`);\n    }\n    // calculate cost after one epoch\n    var costCurrent = this._costFn(batchY, batchPredictions, this._b);\n    // return required data\n    return {\n      costCurrent,\n      gradient\n    };\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} labels \n   * @param {Array|NDArray} residuals \n   */\n  _calculateMetrics(labels, residuals) {\n    // number of observation\n    const n = labels.length;\n    // number of model parameters\n    const k = 1 + (this.intercept != 0) + this._p + this._q;\n    // variance of the residuals white noise\n    this.sigma2 = residuals.power(2).sum() / (n - this._p);\n    // initialise metrics object\n    this.metrics = {};\n    // calculating log likelihood of the data\n    this.metrics.LL = -(n / 2) * (1 + Math.log(2 * Math.PI * this.sigma2));\n    // calculating Akaike's Information Criteria\n    this.metrics.AIC = 2 * (k - this.metrics.LL);\n    // calculating corrected AIC\n    this.metrics.AICc = this.metrics.AIC + 2 * k * (k + 1) / (n - k - 1);\n    // calculating Bayesian Information Criteria\n    this.metrics.BIC = this.metrics.AIC + k * (Math.log(n) - 2);\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} X \n   * @param {number} maxIter \n   * @param {number} stopThreshold \n   * @returns \n   */\n  async fit(X, maxIter = 32, stopThreshold = 1e-6) {\n    this.fitSync(X, maxIter, stopThreshold);\n    return this;\n  }\n\n  /**\n   * TODO forecast needs work; residuals to be drawn from gaussian\n   * @param {number} periods \n   * @returns \n   */\n  forecastSync(periods) {\n    // load required lags for forecasting\n    // var lags = this._lags.slice();\n    var lags = this._lags;\n    \n    // for ARIMA(p, d, 0), initialise residuals to empty array\n    var residuals = [];\n    // for ARIMA(p, d, q)\n    if (this._residuals) {\n      // residuals = this._residuals.slice();\n      residuals = this._residuals;\n    }\n    // predict periods one by one\n    for (let i = 0; i < periods; i++) {\n      // prepare a single record for forecasting\n      // console.log('1')\n      var X = [\n        ...lags.slice(-this._p || lags.length),\n        ...residuals.slice(-this._q)\n      ];\n      // console.log('2')\n      // forecasting\n      var y = this.evaluate(X) + (this.intercept || 0);\n      // adding white noise\n      var [e] = tf.randomNormal([1], 0, Math.sqrt(this.sigma2)).arraySync();\n      y += e;\n      // update lags for next forecast\n      lags.push(y);\n      // for ARIMA(p, d, q) update residuals as well\n      if (residuals.length) {\n        residuals.push(e);\n      }\n    }\n    // the Integration step\n    // https://stackoverflow.com/questions/43563241/numpy-diff-inverted-operation\n    for (let d = this._d - 1; d >= 0; d--) {\n      lags.unshift(this._initialValue[d]);\n      lags = np.cumsum(lags);\n    }\n    // return the forecasted values\n    // return lags.slice(-periods);\n    return lags;\n  }\n\n  /**\n   * \n   * @param {number} periods \n   * @returns \n   */\n  async forecast(periods) {\n    return this.forecastSync(periods);\n  }\n\n  /**\n   * \n   * @param {Array|NDAarray} trueLags \n   */\n  updateSync(trueLags) {\n    // update batch size\n    this._b = (parseInt(this._b / trueLags.length)) ? trueLags.length : this._b;\n    var lags = this._lags.slice();\n    for (let d = this._d - 1; d >= 0; d--) {\n      lags.unshift(this._initialValue[d]);\n      lags = np.cumsum(lags);\n    }\n    trueLags.unshift(...lags);\n    // initialise the fit subroutine\n    var {\n      labels,\n      lags,\n      residuals\n    } = this._fitInit(trueLags, this._residuals);\n    // initialise cost\n    // TODO cost can't be 0 in update\n    var costOld;\n    // loop for specified epoch number\n    // FIXME when researching code in python, this whole function is a workaround\n    for (let epoch = 0; epoch < 4; epoch++) {\n      // run a single epoch, get the final cost & gradient\n      var {\n        costCurrent,\n        gradient\n      } = this._runEpoch(labels, lags, residuals);\n      // early stopping condition\n      if (costOld && super._converged(costOld, costCurrent, 1e-4, gradient)) {\n        break;\n      } else {\n        // update cost for next epoch\n        costOld = costCurrent;\n      }\n    }\n    this._calculateMetrics(labels, residuals);\n    // keeping residuals for forecasting purposes\n    if (this._q) {\n      this._residuals = residuals.slice(-this._q);\n    }\n  }\n\n  /**\n   * \n   * @param {Array|NDArray} trueLags \n   * @returns \n   */\n  async update(trueLags) {\n    return this.updateSync(trueLags);\n  }\n}\n\n\n\nfunction extractIdxTensor1D(listOfObjects) {\n  var res = [];\n  var idx = [];\n\n  for (let i = 0; i < listOfObjects.length; i++) {\n    let obj = listOfObjects[i];\n\n    for (const key in obj) {\n      if (key === \"TimeStamp\") {\n        idx.push(obj[key]);\n      } else {\n        res.push(obj[key]);\n      }\n    }\n  }\n\n  // \tres = tf.tensor1d(res);\n  return [idx, res];\n}\n\n\n// const query = \"SELECT  `TimeStamp`,`currentLuxValue` FROM `GROUP_412` order by `TimeStamp` desc limit 10\";\n// let datalistlenght = 0;\n// TODO train\nconst ts = Array(24).fill(0).map((_, i) => i + Math.random() / 5)\n\n  mod = new AutoRegressionIntegratedMovingAverage([1,1,1]);\n  mod.fitSync(np.array(ts), 8)\n  console.log(JSON.stringify(mod.forecastSync(5)))\n  event.end();\n  // DataListgetAsync('class_room_Adel_11').then(results => {\n  // datalistlenght = results['result'].length;\n\n  // if (datalistlenght == 0) {\n  //   DataListAddAsync('class_room_Adel_11', JSON.stringify(mod))\n\n  // } else {\n  //   DataListupdateAsync('class_room_Adel_11', JSON.stringify(mod), 0)\n  // }\n  // }).then(event.end).catch((err) => {\n  //   // event.error(err);\n  // });\n  \n  \n  \n \n//.then(res => {\n    \n    \n//     DataListgetAsync('class_room_Adel_11').then(results => {\n//       datalistlenght = results['result'].length;\n\n//       if (datalistlenght == 0) {\n//         DataListAddAsync('class_room_Adel_11', JSON.stringify(mod))\n\n//       } else {\n//         DataListupdateAsync('class_room_Adel_11', JSON.stringify(mod), 0)\n//       }\n\n//     }).then(event.end);\n\n//   }\n\n// ).then(event.end).catch((err) => {\n//     // event.error(err);\n//   }\n\n// );"},{"ParameterId":29586,"EventId":212,"ParameterName":"RunAs","ParameterValue":"[[RunAs]]"}],"Groups":[69],"IsSaved":true}